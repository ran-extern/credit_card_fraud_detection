{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import sys\n",
    "from  datetime import datetime\n",
    "from typing import Tuple, Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier,IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, roc_auc_score, average_precision_score, confusion_matrix,roc_curve,precision_recall_curve,accuracy_score,recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import logging\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T02:10:57.314967Z",
     "start_time": "2025-05-01T02:10:57.312777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"../logs/model_building.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ],
   "id": "b4f04cd957be0d60",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configurations\n",
    "class Config:\n",
    "    DATA_PATH = \"../data/processed_data/card_transdata.csv\"\n",
    "    MODELS_DIR = \"../weights/models/\"\n",
    "    REPORTS_DIR = \"../reports/\"\n",
    "    SEED = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    SCORING = 'f1_weighted'\n",
    "    FIG_SIZE = (10, 6)\n",
    "    CV_FOLDS = 10\n",
    "\n"
   ],
   "id": "58e2a7281f4e3344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = Config()\n",
    "\n",
    "os.makedirs(config.MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(config.REPORTS_DIR, exist_ok=True)\n",
    "\n",
    "np.random.seed(config.SEED)"
   ],
   "id": "aca77505e707a011",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==============================\n",
    "# 2. Data Loading\n",
    "# ==============================\n",
    "\n",
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load preprocessed sentiment data.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        logger.info(f\"Data loaded with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {str(e)}\")\n",
    "        raise"
   ],
   "id": "286fbe9aa985699e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = load_data(config.DATA_PATH)\n",
    "df.head(2)"
   ],
   "id": "854dc1fb101a6fae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_features(df:pd.DataFrame)->Tuple[pd.DataFrame,pd.Series]:\n",
    "    '''Separate features and labels from the dataset'''\n",
    "    logger.info(\"Preparing features and labels from the dataset\")\n",
    "    X=df.drop(columns=['fraud'])\n",
    "    y=df['fraud']\n",
    "    return X,y"
   ],
   "id": "5fa10d8c3cc4a5f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X, y = prepare_features(df)\n",
    "y.value_counts().plot.pie(autopct='%0.2f%%')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config.TEST_SIZE, random_state=config.SEED)\n",
    "\n",
    "logger.info(f\"X_train shape: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "y.shape"
   ],
   "id": "22d6251e6f7e459c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==============================\n",
    "# 5. Model Initialization\n",
    "# ==============================\n",
    "\n",
    "def initialize_models() -> Dict[str, BaseEstimator]:\n",
    "    \"\"\"Initialize machine learning models.\"\"\"\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=config.SEED,n_jobs=-1),\n",
    "        \"XGBoost\": XGBClassifier(n_estimators=100, random_state=config.SEED, eval_metric=\"mlogloss\",n_jobs=-1,tree_method=\"hist\"),\n",
    "        \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=config.SEED, class_weight=\"balanced\",n_jobs=-1),\n",
    "    }\n",
    "    return models\n"
   ],
   "id": "78ea04103145b1e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==============================\n",
    "# 6. Model Training and Evaluation\n",
    "# ==============================\n",
    "\n",
    "def evaluate_model(model: Pipeline, X_test: pd.DataFrame, y_test: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model on test data.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"average precision\": average_precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"f1_weighted\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "    }\n",
    "    if y_test.ndim == 2 and y_test.shape[1] > 1:\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "    if y_prob is not None:\n",
    "        metrics[\"roc_auc_ovr\"] = roc_auc_score(y_test, y_prob[:,1], multi_class=\"ovr\", average=\"weighted\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train_and_evaluate_models(X_train: pd.DataFrame, y_train: np.ndarray,\n",
    "                              X_test: pd.DataFrame, y_test: np.ndarray) -> Dict[str, Dict]:\n",
    "    \"\"\"Train and evaluate all models.\"\"\"\n",
    "    results = {}\n",
    "    models = initialize_models()\n",
    "\n",
    "    for name, model in models.items():\n",
    "        logger.info(f\"Training {name}...\")\n",
    "        pipeline = Pipeline([\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        cv_score = cross_val_score(pipeline, X_train, y_train, cv=config.CV_FOLDS, scoring=config.SCORING)\n",
    "        metrics = evaluate_model(pipeline, X_test, y_test)\n",
    "\n",
    "        results[name] = {\n",
    "            \"model\": pipeline,\n",
    "            \"cv_mean\": np.mean(cv_score),\n",
    "            \"cv_std\": np.std(cv_score),\n",
    "            \"test_metrics\": metrics\n",
    "        }\n",
    "\n",
    "        # Confusion matrix\n",
    "        plot_confusion_matrix(pipeline, X_test, y_test, model_name=name)\n",
    "        plot_roc_curve(pipeline, X_test, y_test, model_name=name)\n",
    "        plot_precision_recall_curve(pipeline, X_test, y_test, model_name=name)\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_confusion_matrix(model: Pipeline, X_test: pd.DataFrame, y_test: np.ndarray, model_name: str) -> None:\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    preds = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    plt.figure(figsize=config.FIG_SIZE)\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='d')\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.REPORTS_DIR}/confusion_matrix_{model_name.replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(model: Pipeline, X_test: pd.DataFrame, y_test: np.ndarray, model_name: str) -> None:\n",
    "    \"\"\"Plot ROC curve.\"\"\"\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "    plt.figure(figsize=config.FIG_SIZE)\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name}\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.REPORTS_DIR}/roc_curve_{model_name.replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_precision_recall_curve(model: Pipeline, X_test: pd.DataFrame, y_test: np.ndarray, model_name: str) -> None:\n",
    "    \"\"\"Plot precision-recall curve.\"\"\"\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, preds)\n",
    "    plt.figure(figsize=config.FIG_SIZE)\n",
    "    plt.plot(recall, precision, label=f\"{model_name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.REPORTS_DIR}/precision_recall_curve_{model_name.replace(' ', '_')}.png\")\n",
    "    plt.close()\n"
   ],
   "id": "f002cc7ed3c6ed9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==============================\n",
    "# 7. Model Selection and Saving\n",
    "# ==============================\n",
    "\n",
    "def select_best_model(results: Dict[str, Dict]) -> Tuple[str, Pipeline]:\n",
    "    \"\"\"Select the best performing model.\"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        \"Model\": list(results.keys()),\n",
    "        \"CV_Mean\": [v['cv_mean'] for v in results.values()],\n",
    "        \"CV_Std\": [v['cv_std'] for v in results.values()],\n",
    "        \"Test_precision\": [v['test_metrics']['average precision'] for v in results.values()],\n",
    "        \"Test_F1\": [v['test_metrics']['f1_weighted'] for v in results.values()]\n",
    "    }).sort_values(by=\"CV_Mean\", ascending=False)\n",
    "\n",
    "    logger.info(\"\\n\" + str(summary))\n",
    "    summary.to_csv(f\"{config.REPORTS_DIR}/model_comparison.csv\", index=False)\n",
    "\n",
    "    best_model_name = summary.iloc[0][\"Model\"]\n",
    "    best_model = results[best_model_name][\"model\"]\n",
    "\n",
    "    return best_model_name, best_model\n",
    "\n",
    "def save_model(model: Pipeline, model_name: str) -> None:\n",
    "    \"\"\"Save the model.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    path = f\"{config.MODELS_DIR}/{model_name.replace(' ', '_').lower()}_{timestamp}.pkl\"\n",
    "\n",
    "    with open(path, \"wb\") as f:\n",
    "        joblib.dump(model, f)\n",
    "\n",
    "    logger.info(f\"Model saved at {path}\")\n"
   ],
   "id": "ad7e242dbaa8d125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==============================\n",
    "# 8. Final Execution\n",
    "# ==============================\n",
    "\n",
    "results = train_and_evaluate_models(X_train, y_train, X_test, y_test)\n",
    "best_model_name, best_model = select_best_model(results)\n",
    "save_model(best_model, best_model_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(\"=\"*80)\n"
   ],
   "id": "bad18bfd0545180c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_best_clean_model(best_model, best_model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the best model from clean data into ../models directory with dynamic name.\n",
    "\n",
    "    Args:\n",
    "        best_model: Trained best model object\n",
    "        best_model_name (str): Name of the best model (lowercase, spaces replaced with '_')\n",
    "    \"\"\"\n",
    "    # 1. Format model name\n",
    "    model_name_clean = best_model_name.lower().replace(' ', '_')\n",
    "\n",
    "    # 2. Prepare file path\n",
    "    save_path = f\"../weights/models/best_model_{model_name_clean}.pkl\"\n",
    "\n",
    "    # 3. Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # 4. Save model\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    print(f\"âœ… Best model saved successfully at: {save_path}\")\n",
    "\n",
    "# ===========================\n",
    "# Example Usage (Your Part)\n",
    "# ===========================\n",
    "\n",
    "# Suppose you already have the following variables from your model selection step:\n",
    "# best_model_name = \"Logistic Regression\"\n",
    "# best_model = <your trained Logistic Regression model object>\n",
    "\n",
    "# Now directly call:\n",
    "save_best_clean_model(best_model, best_model_name)\n"
   ],
   "id": "8f5b491c320a32d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    scale_pos_weight=91,  # you can adjust based on your dataset imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss',\n",
    "    tree_method='hist',      # << Use GPU for training\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "param_grid={\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "f1 = make_scorer(f1_score)\n",
    "grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, scoring=f1, n_jobs=-1,verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_xgb = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1 Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on Test\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ],
   "id": "e7e40cf05173a5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6de8b9da516727f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7b313f85ccc41c10",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
